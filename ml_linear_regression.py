# -*- coding: utf-8 -*-
"""ML-Linear Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XjclcKecloWZLHQ4jVdc0o6aeE6-3ioG
"""

# Machine Learning

# Linear Regression Model

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

from IPython.display import Math, Latex, display

"""# DATA

create a data set to perform machine learning analysis on them. To start with working on single feature model with about 100 examples.
"""

w0 = 5
w1 = 3
n = 100

X = 10*np.random.rand(n,)

Y = w0+w1*(X)+np.random.randn(n,)

print(X[:5])
print(Y[:5])

"""Dividing the data to training and testing"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.2, random_state=42)

print('Shape of training data:',X_train.shape)
print('Shape of training label:',y_train.shape)
print('Shape of test data:',X_test.shape)
print('Shape of test label:',y_test.shape)

"""plot the data points"""

plt.title('Data Points')
plt.plot(X_train,y_train,'b.')
plt.xlabel('x_train')
plt.ylabel('y_train')

"""Add dummy feature x0 as 1 to the training set"""

def dummy_feature(x):
  new_x = []
  for i in x:
    new_x.append([1,i])

  return np.array(new_x)

print(dummy_feature(X_train)[:5,])

weight_matrix=np.array([w0,w1])

weight_matrix.shape

dummy_feature(X_train).shape

"""# MODEL

Y = ∑WᵢᵀXᵢ
"""

def prediction(x,w):
  pred=[]
  for i in dummy_feature(x):
    y=0
    for j in range(len(i)):
      y+=(w[j]*i[j])
    pred.append(y)
  return pred

y_hat = prediction(X_train,weight_matrix)

plt.scatter(X_train,y_train, color='purple')
plt.plot(X_train,y_hat, color='black')
plt.title('predicted values')
plt.xlabel('x_train')
plt.ylabel('y_label')

"""#LOSS FUNCTION

SSE = ∑( predicted label ⁱ - actual label ⁱ )

loss = SSE÷2
"""

def loss_function(y_hat,y):
  SSE = (y_hat-y)**2
  loss = sum(SSE)/2
  return loss

SSE = (y_hat-y_train)**2

SSE[:5]

loss = sum(SSE)/2

print(loss_function(y_hat,y_train))

"""# OPTIMIZATION

Method 1 : Normal Calculation


XᵀWX -Xᵀy = 0

W = (XᵀX)\^-1 Xᵀy
"""

weight_norm = np.linalg.pinv(dummy_feature(X_train))@y_train

weight_norm

"""Method 2: Gradient Descent

> STEP1: Random initialization of weight vector

"""

w = np.zeros(weight_matrix.shape)

"""

> STEP2: Iterate until Convergence

>> learning rate α = 0.01, Number of iterations = 50, threshold = 3

"""

alpha = 0.0001
iterations = 2000
threshold = 3

def weight_updates(x,y,w):
  weight = w

  k = 0
  while k<iterations: # to keep the iteration
    print(weight)
    weight_new = [] # create a new list to store the weights
    for l in range(len(weight)): # for the length of weight (eg 0,1)

      loss_weight = 0 # intialize loss_weight to zero

      for i in range(80): # for each examples in training data

        data = dummy_feature(x)

        error = 0 # intizalize error to zero
        for j in range(len(data[i,])): # for each feature in the example

          error+= weight[j]*data[i,j]  # error = weight of the feature * feature value summed over all features

        loss_weight+= (error-y[i])*data[i,l] # loss = error-actual value * the weight corresponding feature value for that example

      weight_new.append(weight[l]-alpha*(loss_weight))


    weight=weight_new

    loss = loss_function(prediction(x,weight),y)


    if loss < threshold:
      break

    k+=1

  return weight,loss

print(weight_updates(X_train,y_train,w))